{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Automatic differentiation with autograd\n",
    "\n",
    "In this step, you learn how to use the MXNet `autograd` package to perform gradient calculations by automatically calculating derivatives.\n",
    "\n",
    "This is helpful because it will help you save time and effort. You train models to get better as a function of experience. Usually, getting better means minimizing a loss function. To achieve this goal, you often iteratively compute the gradient of the loss with respect to weights and then update the weights accordingly. Gradient calculations are straightforward through a chain rule. However, for complex models, working this out manually is challenging.\n",
    "\n",
    "The `autograd` package helps you by automatically calculating derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic use\n",
    "\n",
    "To get started, import the `autograd` package as in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet import autograd\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you could differentiate a function $f(x) = 2 x^2$ with respect to parameter $x$. You can start by assigning an initial value of $x$, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you compute the gradient of $f(x)$ with respect to $x$, you need a place to store it. In MXNet, you can tell an ndarray that you plan to store a gradient by invoking its `attach_grad` method, shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the function $y=f(x)$. To let MXNet store $y$, so that you can compute gradients later, use the following code to put the definition inside an `autograd.record()` scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can invoke back propagation (backprop) by calling `y.backward()`. When $y$ has more than one entry, `y.backward()` is equivalent to `y.sum().backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, verify whether this is the expected output. Note that $y=2x^2$ and $\\frac{dy}{dx} = 4x$, which should be `[[4, 8],[12, 16]]`. Check the automatically computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  8.],\n",
       "       [12., 16.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive into `y.backward()` by first discussing a bit on gradients. First, as we alluded to earlier `y.backward()` is equivalent to `y.sum().backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  8.],\n",
       "       [12., 16.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = np.sum(2 * x * x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to briefly understand this it is helpful to know what our ndarrays essentially are. Basically, our ndarrays are classes with a forward and backward method. Where we the number of args in `backward()` must equal the number of items returned in forward. Additionally, we can customize backward if we so desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MXNet ndarray operations\n",
    "\n",
    "In order to understand the `backward()` method it is beneficial to first understand how can create custom operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_First_Custom_Operation(autograd.Function):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x,y):\n",
    "        self.save_for_backward(x,y) #Save previously calculated values\n",
    "        return 2*x,2*x*y,2*y\n",
    "    def backward(self,dx,dxy,dy):\n",
    "        \"\"\"\n",
    "        The input number of arguments must match the number of outputs from forward.\n",
    "        Furthermore, the number of output arguments must match the number of inputs from forward.\n",
    "        \"\"\"\n",
    "        x,y = self.saved_tensors #Use previously calculated values\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    x = np.random.uniform(-1,1,(2,3))\n",
    "    y = np.random.uniform(-1,1,(2,3))\n",
    "    x.attach_grad()\n",
    "    y.attach_grad()\n",
    "    z = My_First_Custom_Operation()\n",
    "    z1,z2,z3 = z(x,y)\n",
    "    out = z1 + z2 + z3\n",
    "out.backward()\n",
    "print(np.array_equiv(x.asnumpy(),x.asnumpy()))\n",
    "print(np.array_equiv(y.asnumpy(),y.asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could wrap special effects in a normal function like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_first_function(x):\n",
    "    if autograd.is_recording() & (not autograd.is_training()): # Return if we are recording\n",
    "        return(2*x)\n",
    "    elif autograd.is_training(): # Return something else when training\n",
    "        return(4*x)\n",
    "    else:\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "y = my_first_function(x)\n",
    "print(np.array_equiv(y.asnumpy(),x.asnumpy()))\n",
    "with autograd.record(train_mode=False):\n",
    "    y = my_first_function(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "with autograd.record(train_mode=True):# train_mode = True by default\n",
    "    y = my_first_function(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create functions with `autograd.record()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_second_function(x):\n",
    "    with autograd.record():\n",
    "        return(2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "y = my_second_function(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "y = my_second_function(x)\n",
    "with autograd.record():\n",
    "    z = my_second_function(y) + 2\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, MXNet records the execution trace and computes the gradient accordingly.Consider the following function `f` in the following example code. The function doubles the inputs until its `norm` reaches 1000. Then it selects one element depending on the sum of its elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.abs(b).sum() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() >= 0:\n",
    "        c = b[0]\n",
    "    else:\n",
    "        c = b[1]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you record the trace and feed in a random value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(size=2)\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    c = f(a)\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `b` is a linear function of `a`, and `c` is chosen from `b`. The gradient with respect to `a` be will be either `[c/a[0], 0]` or `[0, c/a[1]]`, depending on which element from `b` is picked. You see the results of this example with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == c/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding arguments of Autograd\n",
    "\n",
    "An interesting fact is that `autograd` performs a `sum()` operation on vector valued outputs. You can see that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = np.sum(2 * x * x)\n",
    "    z = 2 * x * x\n",
    "y.backward()\n",
    "y = x.grad\n",
    "z.backward()\n",
    "np.array_equiv(x.grad.asnumpy(),y.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, this may seem unexpected at first the key is to understand what is happening inside of `autograd`. To understand this you have to understand Jacobians and multivariate differentiation. What is the derivative if we had a vector valued output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[-0.46623397 -0.08933949  1.1669002 ]\n",
      " [ 1.2486749   0.11557961 -0.08009136]]\n",
      "[[-0.2496512  -0.80986154  1.567092  ]\n",
      " [-1.7731481   1.854651   -0.90937483]]\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "\n",
      "[[3.533766  3.9106605 5.1669   ]\n",
      " [5.248675  4.1155796 3.9199085]]\n",
      "[[1.7503488  1.1901385  3.567092  ]\n",
      " [0.22685194 3.854651   1.0906252 ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(-1,1,(2,3))\n",
    "y = np.random.uniform(-1,1,(2,3))\n",
    "x.attach_grad()\n",
    "y.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    z = 2*x,2*x*y,2*x+2*y\n",
    "    z1,z2,z3 = z\n",
    "z1.backward()\n",
    "v = x.grad.copy()\n",
    "v1 = y.grad.copy()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "z2.backward()\n",
    "v += x.grad\n",
    "v1 += y.grad\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "z3.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "v += x.grad\n",
    "v1 += y.grad\n",
    "print()\n",
    "print(v)\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see one way we can deal with multiple different gradients is to `sum` like we did with `v,v1`. This is what autograd does by default. As you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    z = 2*x,2*x*y,2*x+2*y\n",
    "    z = np.concatenate([np.expand_dims(zi,axis=0) for zi in z],axis=0)\n",
    "print(z.shape)\n",
    "z.backward()\n",
    "print(np.array_equiv(x.grad.asnumpy(),v.asnumpy()))\n",
    "print(np.array_equiv(y.grad.asnumpy(),v1.asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we were to sum along the first dimension (which autograd assumes is the batch_dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    z = 2*x,2*x*y,2*x+2*y\n",
    "    z = np.sum(np.concatenate([np.expand_dims(zi,axis=0) for zi in z],axis=0),axis=0)\n",
    "print(z.shape)\n",
    "z.backward()\n",
    "print(np.array_equiv(x.grad.asnumpy(),v.asnumpy()))\n",
    "print(np.array_equiv(y.grad.asnumpy(),v1.asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why do we automatically `sum`? Well as you can see we could have a problem where our shapes wouldn't match. Essentially, if we do the operations all seperately then we get 3 different gradients for `x,y`. Which one is correct? Therefore, we have to decide how we should combine those operations. This is where autograd plays a role by default just summing the arguments. We could just perform `mean` if we wanted the average, like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    z = 2*x,2*x*y,2*x+2*y\n",
    "    z = np.mean(np.concatenate([np.expand_dims(zi,axis=0) for zi in z],axis=0),axis=0)\n",
    "print(z.shape)\n",
    "z.backward()\n",
    "print(np.array_equiv(x.grad.asnumpy(),v.asnumpy()))\n",
    "print(np.array_equiv(y.grad.asnumpy(),v1.asnumpy()))\n",
    "v = x.grad.copy()\n",
    "v1 = y.grad.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could pass in an array for the gradient with respect to our current location \"upstream\". Please note, the shape must match the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    z = 2*x,2*x*y,2*x+2*y\n",
    "    z = np.concatenate([np.expand_dims(zi,axis=0) for zi in z],axis=0)\n",
    "print(z.shape)\n",
    "z.backward(np.full(z.shape,1./3.))\n",
    "print(np.array_equiv(x.grad.asnumpy(),v.asnumpy()))\n",
    "print(np.array_equiv(y.grad.asnumpy(),v1.asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 3 values along the dimension 0, so taking a `mean` along this access is the same as summing that axis and multiplying by `1/3`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced MXNet ndarray operations with Autograd\n",
    "\n",
    "Additionally, we can control gradients for different ndarray operations. For instance, perhaps I want to check that the gradients are propogating properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 3*x\n",
    "    y=y.detach()\n",
    "    y.attach_grad()\n",
    "    z = 4*y+2*x\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you want to output multiple values through a custom operation you need to retain the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13608909 -0.21443039  0.8511933 ]\n",
      " [ 0.6721575  -0.8579279  -0.32520765]]\n",
      "[[-0.8257414   0.2963438  -0.9595632 ]\n",
      " [-0.2635169   0.6652397   0.91431034]]\n",
      "[[ 0.13608909 -0.21443039  0.8511933 ]\n",
      " [ 0.6721575  -0.8579279  -0.32520765]]\n",
      "[[-0.8257414   0.2963438  -0.9595632 ]\n",
      " [-0.2635169   0.6652397   0.91431034]]\n"
     ]
    }
   ],
   "source": [
    "class My_First_Custom_Operation(autograd.Function):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x,y):\n",
    "        self.save_for_backward(x,y)\n",
    "        return 2*x,2*x*y,2*y\n",
    "    def backward(self,dx,dxy,dy):\n",
    "        x,y = self.saved_tensors\n",
    "        return x,y\n",
    "    \n",
    "x = np.random.uniform(-1,1,(2,3))\n",
    "y = np.random.uniform(-1,1,(2,3))\n",
    "x.attach_grad()\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    z = My_First_Custom_Operation()\n",
    "    z1,z2,z3 = z(x,y)\n",
    "    z1 = z1 + z2\n",
    "    z3 = z2 + z3\n",
    "z1.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "z3.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Learn how to construct a neural network with the Gluon module: [Step 2: Create a neural network](2-nn.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
