{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --pre mxnet -f https://dist.mxnet.io/python\n",
    "!pip install contextvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mxnet.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary components that are not in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data and algorithms are not the only components that you need to create your trained deep learning model. In this notebook, we will talk about some of the common components that you will use for training your own machine learning models. Here is a list of components that we will talk about in this notebook \n",
    "\n",
    "1. Loss function\n",
    "    1. Built-in\n",
    "    2. Custom\n",
    "2. Optimizers\n",
    "3. Gradients and backpropagation\n",
    "    1. Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx,gluon\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now you have seen how to create an algorithm using mxnet API and the basics of using mxnet. When you start training the ML algorithm, how does it actually learn or train?\n",
    "\n",
    "There are three main components of what happens during training an algorithm. In this notebook, we will talk about these components, namely,\n",
    "\n",
    "1. Loss function which is used to calculate how far the model is from the true distribution\n",
    "2. Autograd, the mxnet auto differentiation tool to calculate the gradients to optimize the parameters\n",
    "3. Optimizer which is used to update the parameters based on an optimization algorithm\n",
    "\n",
    "\n",
    "Lets look at each of them a little closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions are used to train neural networks and help the algorithm learn the data distribution. In a loss function, we compute the difference between output that we get from the neural network and ground truth value. This score is used to update the neural network weights during training. Let's look at a simple example first. \n",
    "\n",
    "Suppose you have a neural network `net` and the data is stored in variable `data`. Let's take 5 total records and the output from the neural network after the first epoch is given by the following variable. The values are in millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00820068],\n",
       "       [0.00382698],\n",
       "       [0.02050169],\n",
       "       [0.00492041],\n",
       "       [0.00608217]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = gluon.nn.Dense(1)\n",
    "net.initialize()\n",
    "\n",
    "nn_input = np.array([1.2,0.56,3.0,0.72,0.89]).reshape(5,1)\n",
    "\n",
    "nn_output = net(nn_input)\n",
    "nn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the ground truth value of the data is stored in `groundtruth_label` is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_label = np.array([[0.0083],\n",
    "                             [0.00382],\n",
    "                             [0.02061],\n",
    "                             [0.00495],\n",
    "                             [0.00639]]).reshape(5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we will use the L2 Loss. L2Loss, also called Mean Squared Error, is a regression loss function that computes the squared distances between the target values and the output of the neural network. It is defined as:\n",
    "\n",
    "$$L = \\frac{1}{2N}\\sum_i{|label_i − pred_i|)^2}$$\n",
    "\n",
    "Compared to L1, L2 loss it is a smooth function and it creates larger gradients for large loss values. However due to the squaring it puts high weight on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9326903e-09, 2.4373411e-11, 5.8656311e-09, 4.3792128e-10,\n",
       "       4.7380531e-08])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def L2Loss(output_values, true_values):\n",
    "    return np.mean((output_values - true_values)**2,axis=1)/2\n",
    "\n",
    "L2Loss(nn_output,groundtruth_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same thing using the mxnet API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9326903e-09, 2.4373411e-11, 5.8656311e-09, 4.3792128e-10,\n",
       "       4.7380531e-08])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet.gluon import nn, loss as gloss\n",
    "loss = gloss.L2Loss()\n",
    "\n",
    "loss(nn_output,groundtruth_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network can improve by iteratively updating its weights to minimise this loss. Some tasks use a combination of multiple loss functions, but often you’ll just use one. MXNet Gluon provides a number of the most commonly used loss functions, and you’ll choose certain loss functions depending on your network and task. Some common task and loss function pairs include:\n",
    "\n",
    "- regression: L1Loss, L2Loss\n",
    "\n",
    "- classification: SigmoidBinaryCrossEntropyLoss, SoftmaxCrossEntropyLoss\n",
    "\n",
    "- embeddings: HingeLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing your Loss functions\n",
    "\n",
    "You can also create custom loss functions using **Loss Blocks**. For more information see []()\n",
    "\n",
    "You can inherit the base `Loss` class and write your own `hybrid_forward` method. The backward propagation will be automatically computed by autograd. However that only holds true if you can build your loss from existing operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9324621e-05, 6.9818925e-06, 1.0831095e-04, 2.9594637e-05,\n",
       "       3.0783284e-04])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet.gluon.loss import Loss\n",
    "\n",
    "class custom_L1_loss(Loss):\n",
    "    def __init__(self,weight=None, batch_axis=0, **kwargs):\n",
    "        super(custom_L1_loss,self).__init__(weight, batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label):\n",
    "        l = np.abs(label - pred)\n",
    "        l = l.reshape(len(l),)\n",
    "        return l\n",
    "    \n",
    "L1 = custom_L1_loss()\n",
    "L1(nn_output,groundtruth_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9324621e-05, 6.9818925e-06, 1.0831095e-04, 2.9594637e-05,\n",
       "       3.0783284e-04])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1=gloss.L1Loss()\n",
    "l1(nn_output,groundtruth_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is the backward step which computes the gradient of the loss with respect to the parameters. In Gluon, this step is achieved by doing the first step in an `autograd.record()` scope to record the computations needed to calculate the loss, and then calling `l.backward()` to compute the gradient of the loss with respect to the parameters.\n",
    "\n",
    "In this step, you learn how to use the MXNet autograd package to perform gradient calculations by automatically calculating derivatives.\n",
    "\n",
    "This is helpful because it will help you save time and effort. You train models to get better as a function of experience. Usually, getting better means minimizing a loss function. To achieve this goal, you often iteratively compute the gradient of the loss with respect to weights and then update the weights accordingly. Gradient calculations are straightforward through a chain rule. However, for complex models, working this out manually is challenging.\n",
    "\n",
    "The autograd package helps you by automatically calculating derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you could differentiate a function $f(x) = 2 x^2$ with respect to parameter $x$. You can start by assigning an initial value of $x$, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you compute the gradient of $f(x)$ with respect to $x$, you need a place to store it. In MXNet, you can tell an nparray that you plan to store a gradient by invoking its `attach_grad()` method, shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the function $y=f(x)$. To let MXNet store $y$, so that you can compute gradients later, use the following code to put the definition inside an `autograd.record()` scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can invoke back propagation (backprop) by calling `y.backward()`. When $y$ has more than one entry, `y.backward()` is equivalent to `y.sum().backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, verify whether this is the expected output. Note that $y=2x^2$ and $\\frac{dy}{dx} = 4x$, which should be `[[4, 8],[12, 16]]`. Check the automatically computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  8.],\n",
       "       [12., 16.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the same example from above, lets look at what happens when you use `backward()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.7877414e-07]\n",
      " [ 4.7713527e-08]\n",
      " [-7.4018578e-07]\n",
      " [-2.0224668e-07]\n",
      " [-2.1036976e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Attaching gradients to the input \n",
    "nn_input.attach_grad()\n",
    "\n",
    "# Computing the gradients \n",
    "with autograd.record():\n",
    "    nn_output = net(nn_input)\n",
    "    L2_loss = loss(nn_output,groundtruth_label)\n",
    "\n",
    "L2_loss.backward()\n",
    "\n",
    "print(nn_input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is how much the parameters are changing based on how far the model is. Optimizer is how the model weights or parameters are updated based on the loss function. In Gluon, this optimization step is performed by the `gluon.Trainer`. \n",
    "\n",
    "Lets look at a basic example of how to call the `gluon.Trainer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                       optimizer='Adam',\n",
    "                       optimizer_params={\n",
    "                           'learning_rate':0.1,\n",
    "                           'wd':0.001\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a **Gluon Trainer** you must provide the trainer object with \n",
    "1. A collection of parameters that need to be learnt. The collection of parameters will the weights and biases of your network that you are training. \n",
    "2. An Optimization algorithm (optimizer) that you want to use for training. This algorithm will be used to update the parameters every training iteration when `trainer.step` is called. For more information, see [optimizers in v1.6 TODO: CHANGE](https://mxnet.apache.org/versions/1.6/api/python/docs/api/optimizer/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0068339]]\n"
     ]
    }
   ],
   "source": [
    "curr_weight = net.weight.data().copy()\n",
    "print(curr_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10660961]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(nn_input)\n",
    "trainer.step(batch_size)\n",
    "print(net.weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00698099]]\n"
     ]
    }
   ],
   "source": [
    "print(curr_weight - net.weight.grad() * 1 / 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
